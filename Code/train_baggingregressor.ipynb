{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this competition a significant delta between local CV and LB scores has been reported in some cases (https://www.kaggle.com/c/trends-assessment-prediction/discussion/153256). We have many features to work with... maybe too many. Reducing variance would seem to be a good thing here and I wanted to investigate the BaggingRegressor for that. The idea is to use the BaggingRegressor to build multiple models, each considering only a fraction of the features, then combine their outputs. From the scikit-learn docs:\n",
    "\n",
    "\"A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\"\n",
    "\n",
    "Ridge regression is known to work well on this dataset, so is used as the base regressor here. The use of the BaggingRegressor is considered as part of a high-performing ensemble, combining SVM and Ridge regression.\n",
    "\n",
    "This notebook is heavily based on @aerdem4's excellent SVM notebook and @tunguz's notebook that adds Ridge regression. Those original notebooks can be found here:\n",
    "https://www.kaggle.com/aerdem4/rapids-svm-on-trends-neuroimaging\n",
    "https://www.kaggle.com/tunguz/rapids-ensemble-for-trends-neuroimaging/\n",
    "\n",
    "## Results\n",
    "\n",
    "After doing an offline sweep of blending weights, the final weights show that for the best local CV, the BaggingRegressor was hardly used for the \"age\" target. However, the BaggingRegressor provided more benefits for the domain variables. In particular for \"domain1_var2\" and \"domain2_var2\" the BaggingRegressor almost completely replaces the basic Ridge regression method.\n",
    "\n",
    "In terms of local CV, the result is almost identical to Bojan's notebook referenced above. On the leaderboard, adding the BaggingRegressor into the ensemble scores 0.1593, an improvement over Bojan's 0.1595. So the local CV to LB delta is successfully reduced, albeit by a little.\n",
    "\n",
    "I find it particularly interesting that only considering small subsets of the features, the BaggingRegressor is competitive for the domain variables but not at all for age.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Install Rapids for faster SVM on GPUs\n",
    "\n",
    "import sys\n",
    "!cp ../input/rapids/rapids.0.13.0 /opt/conda/envs/rapids.tar.gz\n",
    "!cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib/python3.6/site-packages\"] + sys.path\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib/python3.6\"] + sys.path\n",
    "sys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path\n",
    "!cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import warnings\n",
    "from cuml.neighbors import KNeighborsRegressor\n",
    "from cuml import SVR\n",
    "from cuml.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "def metric(y_true, y_pred):\n",
    "    return np.mean(np.sum(np.abs(y_true - y_pred), axis=0)/np.sum(y_true, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.6/site-packages/fsspec/implementations/local.py:33: FutureWarning: The default value of auto_mkdir=True has been deprecated and will be changed to auto_mkdir=False by default in a future release.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "fnc_df = cudf.read_csv(\"../input/trends-assessment-prediction/fnc.csv\")\n",
    "loading_df = cudf.read_csv(\"../input/trends-assessment-prediction/loading.csv\")\n",
    "\n",
    "fnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\n",
    "df = fnc_df.merge(loading_df, on=\"Id\")\n",
    "\n",
    "\n",
    "labels_df = cudf.read_csv(\"../input/trends-assessment-prediction/train_scores.csv\")\n",
    "labels_df[\"is_train\"] = True\n",
    "\n",
    "df = df.merge(labels_df, on=\"Id\", how=\"left\")\n",
    "\n",
    "test_df = df[df[\"is_train\"] != True].copy()\n",
    "df = df[df[\"is_train\"] == True].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giving less importance to FNC features since they are easier to overfit due to high dimensionality.\n",
    "FNC_SCALE = 1/600\n",
    "\n",
    "df[fnc_features] *= FNC_SCALE\n",
    "test_df[fnc_features] *= FNC_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# BaggingRegressor + RAPIDS Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For age:\n",
      "SVR: 0.143988\n",
      "Ridge: 0.143045\n",
      "BaggingRegressor: 0.156089\n",
      "Ensemble: 0.142014\n",
      "\n",
      "For domain1_var1:\n",
      "SVR: 0.151731\n",
      "Ridge: 0.154465\n",
      "BaggingRegressor: 0.151927\n",
      "Ensemble: 0.151191\n",
      "\n",
      "For domain1_var2:\n",
      "SVR: 0.151148\n",
      "Ridge: 0.155502\n",
      "BaggingRegressor: 0.151443\n",
      "Ensemble: 0.150924\n",
      "\n",
      "For domain2_var1:\n",
      "SVR: 0.181716\n",
      "Ridge: 0.186209\n",
      "BaggingRegressor: 0.182352\n",
      "Ensemble: 0.181329\n",
      "\n",
      "For domain2_var2:\n",
      "SVR: 0.176114\n",
      "Ridge: 0.180244\n",
      "BaggingRegressor: 0.176231\n",
      "Ensemble: 0.175668\n",
      "\n",
      "Overall score: 0.157949\n",
      "CPU times: user 8min 13s, sys: 23.2 s, total: 8min 36s\n",
      "Wall time: 8min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# To suppress the \"Expected column ('F') major order, but got the opposite.\" warnings from cudf. It should be fixed properly,\n",
    "# although as the only impact is additional memory usage, I'll supress it for now.\n",
    "warnings.filterwarnings(\"ignore\", message=\"Expected column\")\n",
    "\n",
    "# Take a copy of the main dataframe, to report on per-target scores for each model.\n",
    "# TODO Copy less to make this more efficient.\n",
    "df_model1 = df.copy()\n",
    "df_model2 = df.copy()\n",
    "df_model3 = df.copy()\n",
    "\n",
    "NUM_FOLDS = 7\n",
    "kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n",
    "\n",
    "features = loading_features + fnc_features\n",
    "\n",
    "# Blending weights between the three models are specified separately for the 5 targets. \n",
    "#                                 SVR,  Ridge, BaggingRegressor\n",
    "blend_weights = {\"age\":          [0.4,  0.55,  0.05],\n",
    "                 \"domain1_var1\": [0.55, 0.15,  0.3],\n",
    "                 \"domain1_var2\": [0.45, 0.0,   0.55],\n",
    "                 \"domain2_var1\": [0.55, 0.15,  0.3],\n",
    "                 \"domain2_var2\": [0.5,  0.05,  0.45]}\n",
    "\n",
    "overall_score = 0\n",
    "for target, c, w in [(\"age\", 60, 0.3), (\"domain1_var1\", 12, 0.175), (\"domain1_var2\", 8, 0.175), (\"domain2_var1\", 9, 0.175), (\"domain2_var2\", 12, 0.175)]:    \n",
    "    y_oof = np.zeros(df.shape[0])\n",
    "    y_oof_model_1 = np.zeros(df.shape[0])\n",
    "    y_oof_model_2 = np.zeros(df.shape[0])\n",
    "    y_oof_model_3 = np.zeros(df.shape[0])\n",
    "    y_test = np.zeros((test_df.shape[0], NUM_FOLDS))\n",
    "    \n",
    "    for f, (train_ind, val_ind) in enumerate(kf.split(df, df)):\n",
    "        train_df, val_df = df.iloc[train_ind], df.iloc[val_ind]\n",
    "        train_df = train_df[train_df[target].notnull()]\n",
    "\n",
    "        model_1 = SVR(C=c, cache_size=3000.0)\n",
    "        model_1.fit(train_df[features].values, train_df[target].values)\n",
    "        model_2 = Ridge(alpha = 0.0001)\n",
    "        model_2.fit(train_df[features].values, train_df[target].values)\n",
    "        \n",
    "        ### The BaggingRegressor, using the Ridge regression method as a base, is added here. The BaggingRegressor\n",
    "        # is from sklearn, not RAPIDS, so dataframes need converting to Pandas.\n",
    "        model_3 = BaggingRegressor(Ridge(alpha = 0.0001), n_estimators=30, random_state=42, max_samples=0.3, max_features=0.3)\n",
    "        model_3.fit(train_df.to_pandas()[features].values, train_df.to_pandas()[target].values)\n",
    "\n",
    "        val_pred_1 = model_1.predict(val_df[features])\n",
    "        val_pred_2 = model_2.predict(val_df[features])\n",
    "        val_pred_3 = model_3.predict(val_df.to_pandas()[features])\n",
    "        val_pred_3 = cudf.from_pandas(pd.Series(val_pred_3))\n",
    "        \n",
    "        test_pred_1 = model_1.predict(test_df[features])\n",
    "        test_pred_2 = model_2.predict(test_df[features])\n",
    "        test_pred_3 = model_3.predict(test_df.to_pandas()[features])\n",
    "        test_pred_3 = cudf.from_pandas(pd.Series(test_pred_3))\n",
    "        \n",
    "        val_pred = blend_weights[target][0]*val_pred_1+blend_weights[target][1]*val_pred_2+blend_weights[target][2]*val_pred_3\n",
    "        val_pred = cp.asnumpy(val_pred.values.flatten())\n",
    "        \n",
    "        test_pred = blend_weights[target][0]*test_pred_1+blend_weights[target][1]*test_pred_2+blend_weights[target][2]*test_pred_3\n",
    "        test_pred = cp.asnumpy(test_pred.values.flatten())\n",
    "        \n",
    "        y_oof[val_ind] = val_pred\n",
    "        y_oof_model_1[val_ind] = val_pred_1\n",
    "        y_oof_model_2[val_ind] = val_pred_2\n",
    "        y_oof_model_3[val_ind] = val_pred_3\n",
    "        y_test[:, f] = test_pred\n",
    "        \n",
    "    df[\"pred_{}\".format(target)] = y_oof\n",
    "    df_model1[\"pred_{}\".format(target)] = y_oof_model_1\n",
    "    df_model2[\"pred_{}\".format(target)] = y_oof_model_2\n",
    "    df_model3[\"pred_{}\".format(target)] = y_oof_model_3\n",
    "    test_df[target] = y_test.mean(axis=1)\n",
    "    \n",
    "    score = metric(df[df[target].notnull()][target].values, df[df[target].notnull()][\"pred_{}\".format(target)].values)\n",
    "    overall_score += w*score\n",
    "    \n",
    "    score_model1 = metric(df_model1[df_model1[target].notnull()][target].values, df_model1[df_model1[target].notnull()][\"pred_{}\".format(target)].values)\n",
    "    score_model2 = metric(df_model2[df_model2[target].notnull()][target].values, df_model2[df_model1[target].notnull()][\"pred_{}\".format(target)].values)\n",
    "    score_model3 = metric(df_model3[df_model3[target].notnull()][target].values, df_model3[df_model1[target].notnull()][\"pred_{}\".format(target)].values)\n",
    "\n",
    "    print(f\"For {target}:\")\n",
    "    print(\"SVR:\", np.round(score_model1, 6))\n",
    "    print(\"Ridge:\", np.round(score_model2, 6))\n",
    "    print(\"BaggingRegressor:\", np.round(score_model3, 6))\n",
    "    print(\"Ensemble:\", np.round(score, 6))\n",
    "    print()\n",
    "    \n",
    "print(\"Overall score:\", np.round(overall_score, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>10003_age</td>\n",
       "      <td>55.819954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6672</th>\n",
       "      <td>10003_domain1_var1</td>\n",
       "      <td>50.289878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12549</th>\n",
       "      <td>10003_domain1_var2</td>\n",
       "      <td>59.745923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18426</th>\n",
       "      <td>10003_domain2_var1</td>\n",
       "      <td>48.951002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24303</th>\n",
       "      <td>10003_domain2_var2</td>\n",
       "      <td>56.920075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>10006_age</td>\n",
       "      <td>63.055412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6679</th>\n",
       "      <td>10006_domain1_var1</td>\n",
       "      <td>54.432330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12556</th>\n",
       "      <td>10006_domain1_var2</td>\n",
       "      <td>59.422635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18433</th>\n",
       "      <td>10006_domain2_var1</td>\n",
       "      <td>48.888643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24310</th>\n",
       "      <td>10006_domain2_var2</td>\n",
       "      <td>52.130569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Id  Predicted\n",
       "795             10003_age  55.819954\n",
       "6672   10003_domain1_var1  50.289878\n",
       "12549  10003_domain1_var2  59.745923\n",
       "18426  10003_domain2_var1  48.951002\n",
       "24303  10003_domain2_var2  56.920075\n",
       "802             10006_age  63.055412\n",
       "6679   10006_domain1_var1  54.432330\n",
       "12556  10006_domain1_var2  59.422635\n",
       "18433  10006_domain2_var1  48.888643\n",
       "24310  10006_domain2_var2  52.130569"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = cudf.melt(test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]], id_vars=[\"Id\"], value_name=\"Predicted\")\n",
    "sub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n",
    "\n",
    "sub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\n",
    "assert sub_df.shape[0] == test_df.shape[0]*5\n",
    "sub_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"submission_rapids_ensemble_with_baggingregressor.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
